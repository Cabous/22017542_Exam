---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

#title: "Question 2: Yield Spread"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: yes
Entry1: Question 4 - Volatility Comparison
Entry2: \textbf{Carel Olivier} # textbf for bold
Entry3: 22017542
Uni_Logo: Tex/pca.jpg # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
#Entry4: 
#Entry5: Stellenbosch University
# Entry6: April 2020
# Entry7:
# Entry8:



# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |

---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
library(zoo)
library(factoextra)
library(lubridate)
library(glue)
library(here)
library(fmxdat)

list.files('C:/Users/Cabous/OneDrive/Desktop/22017542_Exam/code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))


```

# Short Introduction

Using the Top 40 Index data I will perform Principal Component Analysis (PCA) as well as a rolling constituent correlation analysis to better understand the concentration and commonality of returns for the J200 index.

```{r}

T40 <- read_rds("data/T40.rds")

```


# Calculate Returns

```{r, include=FALSE}

T40_Q4 <- T40 %>% 
    
    na.locf(.,na.rm=T, 5) %>%
    
    select(date, Tickers, Return, J200) %>%
    
    mutate(Return = Return*J200) %>%
    
    select(date, Tickers, Return) %>% 
    
    group_by(Tickers) %>%
    
    mutate(Tickers = gsub(" SJ Equity", "", Tickers)) %>% 
    
    ungroup()

# PCA using princomp

# prcomp requires wide, numeric data:

return_mat <- T40_Q4 %>% spread(Tickers,Return)

colSums(is.na(T40_Q4))

```

```{r}

options(scipen = 999) # Stop the scientific notation of

return_mat <- impute_missing_returns(return_mat, impute_returns_method = "Drawn_Distribution_Collective", Seed = as.numeric(format( Sys.time(), "%Y%d%H%M")))

return_mat_Nodate <- data.matrix(return_mat[, -1])

# Simple Sample covariance and mean:

Sigma <- RiskPortfolios::covEstimation(return_mat_Nodate)

Mu <- RiskPortfolios::meanEstimation(return_mat_Nodate)

#PCA calculations

pca <- prcomp(return_mat_Nodate,center=TRUE, scale.=TRUE)


# Plot

fviz_screeplot(pca, ncp = 10)
```

```{r}
fviz_pca_var(pca, col.var = "steelblue") + theme_minimal()
```

There are 92 components that explain a percentage of the total variation in the T40 returns. PC1 explains 8 % of the total variance, which is the most out of the 92 PCAâ€™s.

```{r}
fviz_contrib(pca, choice = "var", axes = 1, top = 10)
```

```{r}
fviz_contrib(pca, choice = "var", axes = 2, top = 10)
```


# Calculate rolling constituent correlation

Next I will calculate the rolling average pairwise correlations between all the stocks. That will require calculating the rolling pairwise correlation between all the stock combinations in the index and then take the mean of all those. Thus, I will tackle the problem in bite-sized pieces. The graph below shows the 90-day Mean Rolling Constituent Correlation. I sourced the code I used for the function *rolling_cor_func* , from the following link:

(https://robotwealth.com/rolling-mean-correlations-in-the-tidyverse/)


* Calculate Rolling Correlation

```{r}
df_Q4 <- T40_Q4

#df_Q4 %>% head(5) %>% pretty_table()

# rolling correlation calculation

pairwise_corrs <- rolling_cor_func(df_Q4, 90) %>% 
    
    ungroup() %>%
    
    filter(date > as.Date("2012/03/28"))

# Determine Mean 

Mean_pair_cor <- pairwise_corrs %>% 
    
    group_by(Tickers) %>% 
    
    summarise(Mean_Cor = mean(rollingcor)) 


# Plot Mean over time
mean_pw_cors <- pairwise_corrs %>%
    
  group_by(date) %>%
    
  summarise(mean_pw_corr = mean(rollingcor, na.rm = TRUE))

mean_cor_plot <- mean_pw_cors %>% 
    
    ggplot() + 
    
    geom_line(aes(date, mean_pw_corr), alpha = 0.8, color ="steelblue") +
    
    fmx_cols() + 
    
    theme_fmx(title = ggpts(25)) + 
    
    labs(y = "Rolling Mean Constituent Correlation", 
         x = "", title = "90-day Mean Rolling Constituent Correlation")

finplot(mean_cor_plot)


```





